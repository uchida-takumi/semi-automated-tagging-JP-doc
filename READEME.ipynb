{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このドキュメントの目的\n",
    "\n",
    "半手動で行うテキストへのタグ付けプロセスを、その手順を含めてまとめること。\n",
    "\n",
    "# 必要な環境情報\n",
    "\n",
    "- MacOS\n",
    "- python3.6\n",
    "- anaconda\n",
    "- mecab-python3\n",
    "\n",
    "# 手順の概要\n",
    "\n",
    "### オペレーション_その１\n",
    "\n",
    "1. はじめにドキュメントをファイルに分けて用意します。\n",
    "2. 読み込んだデータをMeCabで形態素解析して品詞分解する。(この品詞分解には独自にカスタマイズしたモジュールを使っています)\n",
    "3. 2で検出した単語（＝品詞の自然活用系（ ×:飲める → ○：飲む ））を、スペース区切り文字列（わかち書き）に変換。\n",
    "4. 3の結果をsklearnのtopic_modelを実行します。\n",
    "5. 各ドキュメントの予測トピックカテゴリをファイルとして出力します。\n",
    "\n",
    "### オペレーション_その2\n",
    "\n",
    "1. オペレーション_その１で出力されたテキストのカテゴライズデータをエクセルで見ながら、手動でtagをつけたファイルを作成します。（半手動部分）\n",
    "2. この時、全てのテキストにtagをつける必要はありません。パッと見て判断がついたものだけにします。\n",
    "3. 手動でtagづけされたデータを再学習します。\n",
    "4. まだtagを手動でつけていないテキストも含めて、tagを予測します。\n",
    "5. 予測タグのデータがファイル出力されますので、間違った予測タグを修正します。\n",
    "6. 修正したファイルを、3に入力し直して、もう一度再学習します。\n",
    "7. 以下、3~6を繰り返して、精度を改善します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from module.mecab_wrapper import keitaiso\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントファイルの読み込み\n",
    "DIR_TEXT_FILE = 'data/text/topic-news'\n",
    "doc_files = [os.path.join(DIR_TEXT_FILE, f) for f in os.listdir(DIR_TEXT_FILE) if re.match('^topic-news-.*.txt$', f)]\n",
    "doc_texts = list()\n",
    "\n",
    "# テキストの前処理関数を定義する\n",
    "def remove_top2line(text):\n",
    "    #　初めの2行は記事のURLと掲載開始日時なので省く\n",
    "    text = ' '.join(text.split('\\n')[2:])    \n",
    "    return text\n",
    "    \n",
    "def text_cleaning(text):\n",
    "    transform_dict = {',':'、', '\\t':' '}\n",
    "    for f,t in transform_dict.items():\n",
    "        text.replace(f, t)\n",
    "    return text\n",
    "\n",
    "for df in doc_files:\n",
    "    with open(df, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = remove_top2line(text)\n",
    "        text = text_cleaning(text)\n",
    "        doc_texts.append(text)\n",
    "\n",
    "# 中身の確認\n",
    "doc_files[0]\n",
    "print( doc_texts[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作のmecab wrapperで形態素解析の設定を行う。\n",
    "# この形態素解析のwrapperでは、動詞は自動的に自然活用形に変換される。\n",
    "\n",
    "kei = keitaiso(use_PoW=['名詞'], stop_words=['彼女', '彼', '私', 'する', '*'])\n",
    "\n",
    "# 以下、実行サンプル\n",
    "kei.tokenize(doc_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_textsをスペース区切り文字列に変換する。\n",
    "convert_wakachi = lambda text:  ' '.join([token[2] for token in kei.tokenize(text)])\n",
    "doc_wakachis = [convert_wakachi(text) for text in doc_texts]\n",
    "\n",
    "doc_wakachis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc - word matrix　を作成する（bags of words, 単なる出現数カウント）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer =  CountVectorizer(\n",
    "    max_df=0.50, # 50%以上のdocに含まれる単語は無視する。\n",
    "    min_df=0.05, # 5%以下のdocにしか含まれない単語は無視する。    \n",
    "    )\n",
    "doc_word_matrix = count_vectorizer.fit_transform(doc_wakachis)\n",
    "\n",
    "# 学習済みの　count_vectorizer はpickleで保存しておく。\n",
    "import pickle\n",
    "file_name = 'data/processed/count_vectorizer.pickle'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic model (LatentDirichletAllocation) のモジュールを読み込む\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "N_TOPIC = 3\n",
    "lda = LatentDirichletAllocation(n_components=N_TOPIC, random_state=0)\n",
    "\n",
    "# 学習して保存しておく。\n",
    "lda.fit(doc_word_matrix)\n",
    "file_name = 'data/processed/lda.pickle'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(lda, f)\n",
    "\n",
    "# 分類された topic\n",
    "import pandas as pd\n",
    "df_doc_topic = pd.DataFrame(\n",
    "    lda.transform(doc_word_matrix), \n",
    "    columns=['topic%02d'%i for i in range(N_TOPIC)]\n",
    ")\n",
    "df_doc_topic['doc_file'] = doc_files\n",
    "df_doc_topic['doc_text'] = doc_texts\n",
    "\n",
    "df_doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doc_topic をファイル出力する。\n",
    "file_name = 'data/df_doc_topic.csv'\n",
    "df_doc_topic.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 以上で、オペレーション_01 は終了です\n",
    "\n",
    "\n",
    "# 続いて、オペレーション_02 を行います\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手続き_02で、手動でtagをつける作業を行います。\n",
    "# tagをつけたデータを読み込みます。\n",
    "# 'data/tag.txt' に手動でタグをつけたデータを読み込みます。\n",
    "TAG_FILE = 'data/tag.txt'\n",
    "try:\n",
    "    df_tag = pd.read_csv(TAG_FILE, sep='\\t')\n",
    "except:\n",
    "    df_tag = pd.read_csv(TAG_FILE, sep='\\t', encoding=\"932\")\n",
    "\n",
    "# このように手動でtagをつけたものです。\n",
    "# 全てのtextにtagをつける必要はありません。\n",
    "df_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後半のほとんどが未記入です。\n",
    "df_tag[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここで、textからベクトルを生成する関数を定義しておきます。\n",
    "\n",
    "text = '''\n",
    "衆議院の総選挙が開始しました。\n",
    "事前の電話調査によれば、与党、野党ともに指示が割れていて予断ができない状態です。\n",
    "'''\n",
    "\n",
    "## テキストをtopicに変換するために必要な学習済みインスタンスを読み込む。\n",
    "file_name = 'data/processed/count_vectorizer.pickle'\n",
    "with open(file_name, 'rb') as f:\n",
    "    count_vectorizer = pickle.load(f)\n",
    "file_name = 'data/processed/lda.pickle'\n",
    "with open(file_name, 'rb') as f:\n",
    "    lda = pickle.load(f)\n",
    "\n",
    "## text_vectorize関数を作成し、textをtopicベクトルに変換する手続きを定義する。\n",
    "def text_vectorize(text):\n",
    "    _text = text_cleaning(text)\n",
    "    token = kei.tokenize(_text)\n",
    "    wakachi = ' '.join([to[2] for to in token])\n",
    "    count_vec = count_vectorizer.transform([wakachi])[0]\n",
    "    topic_vec = lda.transform(count_vec)\n",
    "    # 今回は、topic_vecとcount_vec を変数とする　（tf-idfでもよい）\n",
    "    return np.concatenate([topic_vec, count_vec.toarray()], axis=1)[0]\n",
    "\n",
    "## このように、入力されたtextをtopicベクトルとして出力することができるようになりました。\n",
    "x = text_vectorize(text)\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手動でつけたタグを学習します。\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "estimater_dict = dict()\n",
    "tags    = [col for col in df_tag.columns if re.match('^tag_', col)] \n",
    "topics = [col for col in df_tag.columns if re.match('^topic', col)] \n",
    "for tag in tags:\n",
    "    # tag を未記入の部分は学習から除外します\n",
    "    train_index = np.logical_not(np.isnan(df_tag[tag]))\n",
    "    y = df_tag.loc[train_index, tag]\n",
    "    # 上記で定義したベクトル化関数を使います。\n",
    "    X = [text_vectorize(text) for text in df_tag.loc[train_index, 'doc_text']]\n",
    "    estimator = GradientBoostingClassifier()\n",
    "    estimator.fit(X, y)\n",
    "    estimater_dict[tag] = estimator\n",
    "\n",
    "# タグごとの学習済みセットが出来上がります。\n",
    "estimater_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# では新しいtextに対して予測させてみます。\n",
    "text = '''\n",
    "衆議院の総選挙が開始しました。\n",
    "事前の電話調査によれば、与党、野党ともに指示が割れていて予断ができない状態です。\n",
    "'''\n",
    "def predict(text):\n",
    "    x = text_vectorize(text)\n",
    "    predict_result = {}\n",
    "    for tag, estimator in estimater_dict.items():\n",
    "        predict_result[tag] = estimator.predict_proba([x])[0][1]\n",
    "\n",
    "    return predict_result\n",
    "\n",
    "predict(text) # 政治は予測できているようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "先日、サッカーの世界大会がありました。\n",
    "日本代表チームは予選突破し、決勝トーナメントへ駒を進めました。しかし、惜しくも３回戦で敗退しました。\n",
    "次は五輪オリンピック予選に向けて、調整に入ります。\n",
    "'''\n",
    "predict(text) # 海外は予測できているけれど、スポーツはまだ不十分です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# より予測精度を上げるためには、1)モデルの改善、2)データの改善を行います。\n",
    "# ここではデータの改善に取り組んでみます。　モデルはいつでも変更できますが、tagつきデータはどこにもありません。\n",
    "\n",
    "# まずは、現状の予測 数値を　df_tag に追加します。\n",
    "for index, row in df_tag.iterrows():\n",
    "    predicted_dict = predict(row['doc_text'])\n",
    "    for tag, val in predicted_dict.items():\n",
    "        df_tag.loc[index, 'predict_'+tag] = val\n",
    "\n",
    "df_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測は、　tag づけしていないデータにもつきます。\n",
    "df_tag[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ファイルとして出力します。\n",
    "file_name = 'data/tag_predict.csv'\n",
    "df_tag.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 後は、、、\n",
    "\n",
    "上記で出力された　data/tag_predict.csv　を目視で確認して、予測が間違っている部分の　tag を手動で入力し直しましょう。\n",
    "\n",
    "（予測が正しい場合を、再入力してもよいですが、あまり効果はありません。現状の学習の強化になるだけです）\n",
    "\n",
    "入力し直したデータを、　data/tag_predict.txt に上書きします。\n",
    "\n",
    "そして、また再学習することで精度が強化されていきます。\n",
    "\n",
    "こうして、「正しいデータを、効率的に増やしていきます」\n",
    "\n",
    "後は、学習モデルをより高度なものに変更してみてもよいでしょう。\n",
    "\n",
    "RNNでもBERTなど、自然言語の判別アルゴリズムは2019年の現在では頻繁にアップデートされています。\n",
    "\n",
    "それらは、tagづけされたデータが出来上がってから改めて再学習することができます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
